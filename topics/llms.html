<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="shortcut icon" href="../bootstrap/H.jpg">
	 
    <title>LLM</title>

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-T3c6CoIi6uLrA9TneNEoa7RxnatzjcDSCmG1MXxSR1GAsXEV/Dwwykc2MPK8M2HN" crossorigin="anonymous">
  </head>
  <body">
    <div class="page-header pt-3"></div>
    <hr>

    <nav class="navbar navbar-expand-lg navbar-light bg-light">
      <div class="container">
          <a class="navbar-brand" href="#">Navbar</a>
          <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
          </button>
          <div class="collapse navbar-collapse" id="navbarNav">
            <ul class="navbar-nav ml-auto">
              <li class="nav-item">
                <a class="nav-link" href="./llms.html">&nbsp; LLM &nbsp; </a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="./robust.html">&nbsp; Robust &nbsp; </a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="./grl.html">&nbsp; GRL &nbsp; </a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="./apx.html">&nbsp; APX &nbsp; </a>
              </li>
            </ul>
          </div>
      </div>
    </nav>
    <hr>

    <section id="abstract" class="container my-5">
        <h3>LLM Understanding and Evaluation</h3>
        <p>NeurIPS (1)</p>
    </section>

    <div class="container my-5">
      <div class="row">
        <!-- Card for Paper 1 -->
        <div class="col-lg-6 col-md-6 mb-4">
          <div class="card h-100">
            <img src="llms/NeurIPS-2024.png" class="card-img-top" alt="The main results of PertEval">
            <div class="card-body">
              <h6 class="card-title">[NeurIPS-2024] PertEval: Unveiling Real Knowledge Capacity of LLMs with Knowledge-Invariant Perturbations</h6>
              <p class="card-text" style="font-size: 14px;">
                Expert-designed close-ended benchmarks serve as vital tools in assessing the
                knowledge capacity of large language models (LLMs). Despite their widespread
                use, concerns have mounted regarding their reliability due to limited test scenarios
                and an unavoidable risk of data contamination. To rectify this, we present PertEval,
                a toolkit devised for in-depth probing of LLMs’ knowledge capacity through
                knowledge-invariant perturbations. These perturbations employ human-like
                restatement techniques to generate on-the-fly test samples from static benchmarks,
                meticulously retaining knowledge-critical content while altering irrelevant details.
                Our toolkit further includes a suite of response consistency analyses that compare
                performance on raw vs. perturbed test sets to precisely assess LLMs’ genuine
                knowledge capacity. Six representative LLMs are re-evaluated using PertEval.
                Results reveal significantly inflated performance of the LLMs on raw benchmarks,
                including an absolute 25.8% overestimation for GPT-4. Additionally, through
                a nuanced response pattern analysis, we discover that PertEval retains LLMs’
                uncertainty to specious knowledge, and reveals their potential rote memorization
                to correct options which leads to overestimated performance. We also find that
                the detailed response consistency analyses by PertEval could illuminate various
                weaknesses in existing LLMs’ knowledge mastery and guide the development of
                refinement. Our findings demonstrate the effectiveness of PertEval in promoting
                the trustworthiness of LLM evaluation, providing insights for advancing more
                robust and genuinely knowledgeable LLMs.</p>
            </div>
            <div class="card-footer">
              <a target="_blank" href="https://arxiv.org/abs/2405.19740" class="btn btn-primary">Read More</a>
            </div>
          </div>
        </div>
      </div>
    </div>



    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-C6RzsynM9kWDrMNeT87bh95OGNyZPhcTNXj1NW7RuBCsyN/o0jlpcV8Qyq46cDfL" crossorigin="anonymous"></script>
  </body>
</html>